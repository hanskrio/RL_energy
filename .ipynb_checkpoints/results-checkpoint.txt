Baseline (one week simulation):
KPIs for the baseline controller:
tdis_tot: 4.122446923161823
idis_tot: 0
ener_tot: 2.092411874785141
cost_tot: 0.530426410258033
emis_tot: 0.34943278308911874
pele_tot: 0.018979879029778473
pgas_tot: None
pdih_tot: None
time_rat: 0.000438785287275841

(two week simulation):
KPIs for the baseline controller:
tdis_tot: 10.518481491630794
idis_tot: 0
ener_tot: 3.569970987031343
cost_tot: 0.9049876452124441
emis_tot: 0.5961851548342336
pele_tot: 0.018853385753559982
pgas_tot: None
pdih_tot: None
time_rat: 0.0004343239547403693


Q-learning agent training for 10 steps (one week simulation):
model = Q_Learning_Agent(env, eps_min=0.01, eps_decay=0.001, alpha=0.1, gamma=0.9)
model.learn(total_episodes=10)
Results
{'tdis_tot': 1.741814571742692,
 'idis_tot': 0,
 'ener_tot': 0.17501251008682203,
 'cost_tot': 0.0443656713070094,
 'emis_tot': 0.02922708918449929,
 'pele_tot': 0.019907644464042158,
 'pgas_tot': None,
 'pdih_tot': None,
 'time_rat': 0.00043631156861494136}

DQN training for 10 steps (one day simulation):
model = DQN('MlpPolicy', env, verbose=1, gamma=0.99,
            learning_rate=5e-4, batch_size=24, seed=123456,
            buffer_size=365*24, learning_starts=24, train_freq=1)
Results
{'tdis_tot': 8.370831530559261,
 'idis_tot': 0,
 'ener_tot': 0.3965284452967778,
 'cost_tot': 0.1005199608827332,
 'emis_tot': 0.06622025036456192,
 'pele_tot': 0.021041628867335238,
 'pgas_tot': None,
 'pdih_tot': None,
 'time_rat': 0.001018399723150112}

DQN training for 100 steps (one day simulation)
model = DQN('MlpPolicy', env, device=device, verbose=1, gamma=0.99,
            learning_rate=5e-4, batch_size=24, seed=123456,
            buffer_size=365*24, learning_starts=24, train_freq=1)

{'tdis_tot': 37.15312778581881,
 'idis_tot': 0,
 'ener_tot': 0.4643638858761871,
 'cost_tot': 0.11771624506961344,
 'emis_tot': 0.07754876894132326,
 'pele_tot': 0.02370474504325742,
 'pgas_tot': None,
 'pdih_tot': None,
 'time_rat': 0.0010514094708142458}

PPO  training for 1 iteration
model = PPO('MlpPolicy', env, verbose=1, device=device,
            learning_rate=3e-4, batch_size=32, n_steps=512,
            gamma=0.99, seed=seed)
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -22.1    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 1        |
|    time_elapsed    | 1883     |
|    total_timesteps | 512      |
---------------------------------
- The mean episode length is 24 timesteps. This means that, on average, each episode lasts 24 timesteps before terminating
- The mean episode reward is -22.1. This negative reward suggests that the agent is currently not performing well and is likely far from optimizing its actions in the environment.
- You are only on the first iteration of training. PPO usually takes multiple iterations to converge
- A total of 512 timesteps have been simulated. Depending on your environment and the complexity of the task, this is also a relatively small number of timesteps for PPO training.
- The time elapsed since starting the training is 1883 seconds (~31 minutes), which seems quite long for 512 timesteps


PPO  training for 2 iterations w/ reduced observation space
