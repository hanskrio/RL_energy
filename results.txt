Baseline (one week simulation):
KPIs for the baseline controller:
tdis_tot: 4.122446923161823
idis_tot: 0
ener_tot: 2.092411874785141
cost_tot: 0.530426410258033
emis_tot: 0.34943278308911874
pele_tot: 0.018979879029778473
pgas_tot: None
pdih_tot: None
time_rat: 0.000438785287275841

(two week simulation):
KPIs for the baseline controller:
tdis_tot: 10.518481491630794
idis_tot: 0
ener_tot: 3.569970987031343
cost_tot: 0.9049876452124441
emis_tot: 0.5961851548342336
pele_tot: 0.018853385753559982
pgas_tot: None
pdih_tot: None
time_rat: 0.0004343239547403693

(one day simulation):
KPIs for the baseline controller:
{'tdis_tot': 0.7698551272370742,
 'idis_tot': 0,
 'ener_tot': 0.30687116488485205,
 'cost_tot': 0.07839112365209558,
 'emis_tot': 0.051247484535770306,
 'pele_tot': 0.018077890262514152,
 'pgas_tot': None,
 'pdih_tot': None,
 'time_rat': 0.0023178886539406245}


Q-learning agent training for 10 steps (one week simulation):
model = Q_Learning_Agent(env, eps_min=0.01, eps_decay=0.001, alpha=0.1, gamma=0.9)
model.learn(total_episodes=10)
Results
{'tdis_tot': 1.741814571742692,
 'idis_tot': 0,
 'ener_tot': 0.17501251008682203,
 'cost_tot': 0.0443656713070094,
 'emis_tot': 0.02922708918449929,
 'pele_tot': 0.019907644464042158,
 'pgas_tot': None,
 'pdih_tot': None,
 'time_rat': 0.00043631156861494136}

DQN training for 10 steps (one day simulation):
model = DQN('MlpPolicy', env, verbose=1, gamma=0.99,
            learning_rate=5e-4, batch_size=24, seed=123456,
            buffer_size=365*24, learning_starts=24, train_freq=1)
Results
{'tdis_tot': 8.370831530559261,
 'idis_tot': 0,
 'ener_tot': 0.3965284452967778,
 'cost_tot': 0.1005199608827332,
 'emis_tot': 0.06622025036456192,
 'pele_tot': 0.021041628867335238,
 'pgas_tot': None,
 'pdih_tot': None,
 'time_rat': 0.001018399723150112}

DQN training for 10 steps (one week simulation):
model = DQN('MlpPolicy', env, verbose=1, gamma=0.99,
            learning_rate=5e-4, batch_size=64, 
            buffer_size=365*64, learning_starts=24, train_freq=1, seed=4)
Results
cost_tot: 0.9508707261396719
emis_tot: 0.6264118787586799
ener_tot: 3.750969333884307
idis_tot: 0.0
pdih_tot: None
pele_tot: 0.025508724067640307
pgas_tot: None
tdis_tot: 2318.7248982873375
time_rat: 6.039795126822974e-05

DQN training for 10 steps (two week simulation):
model = DQN('MlpPolicy', env, verbose=1, gamma=0.99,
            learning_rate=5e-4, batch_size=64, 
            buffer_size=365*64, learning_starts=24, train_freq=1, seed=4)
Results
cost_tot: 1.87880348731566
emis_tot: 1.2377127510126833
ener_tot: 7.41145359887835
idis_tot: 0.0
pdih_tot: None
pele_tot: 0.025508724067640307
pgas_tot: None
tdis_tot: 5032.458279499428
time_rat: 6.061462086824635e-05


DQN training for 100 steps (one day simulation)
model = DQN('MlpPolicy', env, device=device, verbose=1, gamma=0.99,
            learning_rate=5e-4, batch_size=24, seed=123456,
            buffer_size=365*24, learning_starts=24, train_freq=1)

{'tdis_tot': 37.15312778581881,
 'idis_tot': 0,
 'ener_tot': 0.4643638858761871,
 'cost_tot': 0.11771624506961344,
 'emis_tot': 0.07754876894132326,
 'pele_tot': 0.02370474504325742,
 'pgas_tot': None,
 'pdih_tot': None,
 'time_rat': 0.0010514094708142458}

DQN training for 10,000 steps (one day simulation)
model = DQN('MlpPolicy', env, device=device, verbose=1, gamma=0.99,
            learning_rate=5e-4, batch_size=24, seed=123456,
            buffer_size=365*24, learning_starts=24, train_freq=1)

cost_tot: 0.07001185612215581
emis_tot: 0.04612220896410265
ener_tot: 0.27618089200061463
idis_tot: 0.0
pdih_tot: None
pele_tot: 0.019712815382475407
pgas_tot: None
tdis_tot: 1.0721725439948198
time_rat: 5.158490028934203e-05

Start:
---------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -9.25    |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 0        |
|    time_elapsed     | 146      |
|    total_timesteps  | 96       |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.216    |
|    n_updates        | 71       |
----------------------------------

End:
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.366   |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 416      |
|    fps              | 0        |
|    time_elapsed     | 13413    |
|    total_timesteps  | 9984     |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.14e-05 |
|    n_updates        | 9959     |
----------------------------------



DQN training for 100,000 steps (one day simulation)
model = DQN('MlpPolicy', env, verbose=1, gamma=0.99,
            learning_rate=5e-4, batch_size=64, 
            buffer_size=365*64, learning_starts=24, train_freq=1, seed=1337)
{
  "cost_tot": 0.04546327362193179,
  "emis_tot": 0.02995016447677558,
  "ener_tot": 0.1793423022561412,
  "idis_tot": 0.0,
  "pdih_tot": null,
  "pele_tot": 0.022898386745500217,
  "pgas_tot": null,
  "tdis_tot": 0.0,
  "time_rat": 5.333136533193542e-05
}

PPO  training for 1 iteration
model = PPO('MlpPolicy', env, verbose=1, device=device,
            learning_rate=3e-4, batch_size=32, n_steps=512,
            gamma=0.99, seed=seed)
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -22.1    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 1        |
|    time_elapsed    | 1883     |
|    total_timesteps | 512      |
---------------------------------
- The mean episode length is 24 timesteps. This means that, on average, each episode lasts 24 timesteps before terminating
- The mean episode reward is -22.1. This negative reward suggests that the agent is currently not performing well and is likely far from optimizing its actions in the environment.
- You are only on the first iteration of training. PPO usually takes multiple iterations to converge
- A total of 512 timesteps have been simulated. Depending on your environment and the complexity of the task, this is also a relatively small number of timesteps for PPO training.
- The time elapsed since starting the training is 1883 seconds (~31 minutes), which seems quite long for 512 timesteps


PPO  training for 2 iterations w/ reduced observation space
